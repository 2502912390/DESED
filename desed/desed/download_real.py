# -*- coding: utf-8 -*-
import functools
import glob
import inspect
import os
import shutil
import warnings
from contextlib import closing
from multiprocessing import Pool

import pandas as pd
import youtube_dl
from dcase_util.containers import AudioContainer
from tqdm import tqdm
from youtube_dl.utils import ExtractorError, DownloadError

from .logger import create_logger, DesedWarning
from .utils import create_folder, download_file


# Needed to not print warning from youtube_dl which cause breaks in the progress bar.
class MyLogger(object):
    def debug(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        pass


def download_audioset_file(filename, result_dir, platform="youtube", tmp_folder="tmp"):
    """ download a file from youtube given an audioSet filename. (It takes only a part of the file thanks to
    information provided in the filename)
    Args:
        filename : str, AudioSet filename to download.
        result_dir : str, result directory which will contain the downloaded file.
        platform: str, name of the platform, here youtube or vimeo.
        tmp_folder: str, the directory in which to download the temporary file generated by youtube_dl.
    Return:
        list, Empty list if the file is downloaded, otherwise contains the filename and the error associated

    """
    logger = create_logger(__name__ + "/" + inspect.currentframe().f_code.co_name)
    tmp_filename = ""
    fname_no_ext = os.path.splitext(filename)[0]
    segment_start = fname_no_ext.split("_")[-2]
    segment_end = fname_no_ext.split("_")[-1]
    audio_container = AudioContainer()

    # Define download parameters
    ydl_opts = {
        "format": "bestaudio/best",
        "outtmpl": tmp_folder + "%(id)s.%(ext)s",
        "noplaylist": True,
        "quiet": True,
        "prefer_ffmpeg": True,
        "logger": MyLogger(),
        "audioformat": "wav",
    }

    if platform.lower() == "youtube":
        query_id = filename[1:12]  # Remove the Y in front of the file.
        baseurl = "https://www.youtube.com/watch?v="
    elif platform.lower() == "vimeo":
        query_id = filename.split("_")[0]
        baseurl = "https://vimeo.com/"
    else:
        raise NotImplementedError("platform can only be vimeo or youtube")

    if not os.path.isfile(os.path.join(result_dir, filename)):
        try:
            logger.debug(filename)
            # Download file
            with youtube_dl.YoutubeDL(ydl_opts) as ydl:
                meta = ydl.extract_info(f"{baseurl}{query_id}", download=True)

            audio_formats = [f for f in meta["formats"] if f.get("vcodec") == "none"]
            if audio_formats is []:
                return [filename, "no audio format available"]
            # get the best audio format
            best_audio_format = audio_formats[-1]

            tmp_filename = tmp_folder + query_id + "." + best_audio_format["ext"]
            audio_container.load(
                filename=tmp_filename,
                fs=44100,
                res_type="kaiser_best",
                start=float(segment_start),
                stop=float(segment_end),
                auto_trimming=True,
            )

            # Save segmented audio
            audio_container.filename = filename
            audio_container.detect_file_format()
            audio_container.save(filename=os.path.join(result_dir, filename))

            # Remove temporary file
            os.remove(tmp_filename)
            return []

        except (KeyboardInterrupt, SystemExit):
            # Remove temporary files and current audio file.
            for fpath in glob.glob(tmp_folder + query_id + "*"):
                os.remove(fpath)
            raise

        # youtube-dl error, file often removed, IO Error is for AudioContainer error if length of file is different.
        except (ExtractorError, DownloadError, IOError) as e:
            if os.path.exists(tmp_filename):
                os.remove(tmp_filename)

            return [filename, str(e)]

        # multiprocessing can give this error
        except (IndexError, ValueError) as e:
            if os.path.exists(tmp_filename):
                os.remove(tmp_filename)
            logger.info(filename)
            logger.info(str(e))
            return [filename, "Index Error"]
    else:
        logger.debug(filename, "exists, skipping")
        return []


def download_audioset_files(
    filenames,
    result_dir,
    n_jobs=1,
    chunk_size=10,
    missing_files_tsv="..",
    platform="youtube",
):
    """ download files in parallel from youtube given a tsv file listing files to download.
    It also stores not downloaded files with their associated error in "missing_files_[tsv_file].tsv"

       Args:
           filenames : pandas Series, named "filename" listing AudioSet filenames to download
           result_dir : str, result directory which will contain downloaded files
           n_jobs : int, number of download to execute in parallel
           chunk_size : int, number of files to download before updating the progress bar. Bigger it is, faster it goes
                because data is filled in memory but progress bar only updates after a chunk is finished.
           missing_files_tsv: str, path of the tsv which will contain the files that couldn't have been downloaded.
           platform: str, the platform the filenames are coming from "youtube" or "vimeo"

       Returns:
           missing_files : pandas.DataFrame, files not downloaded whith associated error.

       """
    warnings.filterwarnings("ignore")
    create_folder(result_dir)
    TMP_FOLDER = "tmp/"
    create_folder(TMP_FOLDER)

    p = None
    files_error = []
    try:
        if n_jobs == 1:
            for filename in tqdm(filenames):
                files_error.append(
                    download_audioset_file(filename, result_dir, platform)
                )
        # multiprocessing
        else:
            with closing(Pool(n_jobs)) as p:
                # Put result_dir and platform as constants variable with result_dir in download_file
                download_file_alias = functools.partial(
                    download_audioset_file,
                    result_dir=result_dir,
                    platform=platform,
                    tmp_folder=TMP_FOLDER,
                )

                for val in tqdm(
                    p.imap_unordered(download_file_alias, filenames, chunk_size),
                    total=len(filenames),
                ):
                    files_error.append(val)

        # Store files which gave error
        missing_files = pd.DataFrame(files_error).dropna()
        if not missing_files.empty:
            # Save missing_files to be able to ask them
            missing_files.columns = ["filename", "error"]
            missing_files.to_csv(missing_files_tsv, index=False, sep="\t")
            warnings.warn(
                f"There are missing files at {missing_files_tsv}, \n"
                f"see info on https://github.com/turpaultn/desed on how to get them",
                DesedWarning,
            )

    except KeyboardInterrupt:
        if p is not None:
            p.terminate()
        raise KeyboardInterrupt

    if os.path.exists(TMP_FOLDER):
        shutil.rmtree(TMP_FOLDER)
    warnings.resetwarnings()
    return missing_files


def download_audioset_files_from_csv(
    tsv_path, result_dir, missing_files_tsv=None, n_jobs=3, chunk_size=10
):
    """ Download audioset files from a tsv_path containing a column "filename"

    Args:
        tsv_path: str, the path to the tsv (tab separated values) containing the column 'filename' with the files to
            download
        result_dir: str, the directory in which to download the files.
        missing_files_tsv: str, path of the tsv which will contain the files that couldn't have been downloaded.
        n_jobs : int, number of download to execute in parallel
        chunk_size : int, number of files to download before updating the progress bar. Bigger it is, faster it goes
            because data is filled in memory but progress bar only updates after a chunk is finished.

    Returns:

    """
    logger = create_logger(__name__ + "/" + inspect.currentframe().f_code.co_name)
    logger.info(f"downloading data from: {tsv_path}")
    if missing_files_tsv is None:
        missing_files_tsv = (
            "missing_files" + os.path.basename(os.path.splitext(tsv_path)[0]) + ".tsv"
        )
    # read metadata file and get only one filename once
    df = pd.read_csv(tsv_path, header=0, sep="\t")
    filenames_test = df["filename"].drop_duplicates()

    download_audioset_files(
        filenames_test,
        result_dir,
        n_jobs=n_jobs,
        chunk_size=chunk_size,
        missing_files_tsv=missing_files_tsv,
    )
    logger.info("###### DONE #######")


def download_eval_public(dataset_folder):
    """ Download the public eval part of desed dataset from Zenodo.

    Args:
        dataset_folder: str, the path to the root of the dataset where to download the evaluation files (this folder
            contains audio and metadata folders).

    Returns:

    """
    archive_folder = os.path.join("tmp", "zip_public")
    create_folder(archive_folder)
    create_folder(dataset_folder)
    url_public_eval = (
        f"https://zenodo.org/record/4560759/files/DESED_public_eval.tar.gz?download=1"
    )
    fpath_tar = os.path.join(archive_folder, "DESED_public_eval.tar.gz")
    download_file(url_public_eval, fpath_tar)
    shutil.unpack_archive(fpath_tar, dataset_folder)
    shutil.rmtree(archive_folder)


def download_audioset_data(
    dataset_folder,
    weak=True,
    unlabel_in_domain=True,
    validation=True,
    n_jobs=3,
    chunk_size=10,
):
    """ Download the DESED dataset files from Audioset.

    Args:
        dataset_folder: str, the path to the root of the dataset where to download the evaluation files (this folder
            contains audio and metadata folders).
        weak: bool, whether to download the weak set or not.
        unlabel_in_domain: bool, whether to download the unlabel_in_domain set or not.
        validation: bool, whether to download the validation set or not.
        n_jobs : int, number of download to execute in parallel
        chunk_size : int, number of files to download before updating the progress bar. Bigger it is, faster it goes
            because data is filled in memory but progress bar only updates after a chunk is finished.

    Returns:

    """
    logger = create_logger(__name__ + "/" + inspect.currentframe().f_code.co_name)
    basedir_missing_files = "missing_files"
    create_folder(basedir_missing_files)
    create_folder(dataset_folder)

    # Metadata:
    archive_folder = os.path.join("tmp", "audioset_metadata")
    create_folder(archive_folder)
    url_metadata = (
        f"https://zenodo.org/record/4560857/files/audioset_metadata.tar.gz?download=1"
    )
    fpath_tar = os.path.join(archive_folder, "audioset_metadata.tar.gz")
    download_file(url_metadata, fpath_tar)
    shutil.unpack_archive(fpath_tar, dataset_folder)
    shutil.rmtree(archive_folder)

    if weak:
        logger.info("Downloading Weakly labeled data...")
        download_audioset_files_from_csv(
            os.path.join(dataset_folder, "metadata", "train", "weak.tsv"),
            os.path.join(dataset_folder, "audio", "train", "weak"),
            missing_files_tsv=os.path.join(
                basedir_missing_files, "missing_files_" + "weak" + ".tsv"
            ),
            n_jobs=n_jobs,
            chunk_size=chunk_size,
        )

    if unlabel_in_domain:
        logger.info("Downloading Unlabeled (in_domain) labeled data...")
        download_audioset_files_from_csv(
            os.path.join(dataset_folder, "metadata", "train", "unlabel_in_domain.tsv"),
            os.path.join(dataset_folder, "audio", "train", "unlabel_in_domain"),
            missing_files_tsv=os.path.join(
                basedir_missing_files, "missing_files_" + "unlabel_in_domain" + ".tsv"
            ),
            n_jobs=n_jobs,
            chunk_size=chunk_size,
        )

    if validation:
        logger.info("Downloading validation, strongly labeled data...")
        download_audioset_files_from_csv(
            os.path.join(dataset_folder, "metadata", "validation", "validation.tsv"),
            os.path.join(dataset_folder, "audio", "validation"),
            missing_files_tsv=os.path.join(
                basedir_missing_files, "missing_files_" + "validation" + ".tsv"
            ),
            n_jobs=n_jobs,
            chunk_size=chunk_size,
        )

    logger.info(
        f"Please check your missing_files: {basedir_missing_files}, "
        f"you can relaunch 'download_audioset_sets' to try to recude them, "
        "then, send these missing_files to "
    )


def download_real(
    dataset_folder,
    weak=True,
    unlabel_in_domain=True,
    validation=True,
    eval=True,
    n_jobs=3,
    chunk_size=10,
):
    """ Download the DESED real part of the dataset.

    Args:
        dataset_folder: str, the path to the root of the dataset where to download the evaluation files (this folder
            contains audio and metadata folders).
        weak: bool, whether to download the weak set or not.
        unlabel_in_domain: bool, whether to download the unlabel_in_domain set or not.
        validation: bool, whether to download the validation set or not.
        eval: bool, whether to download the public eval set or not.
        n_jobs : int, number of download to execute in parallel
        chunk_size : int, number of files to download before updating the progress bar. Bigger it is, faster it goes
            because data is filled in memory but progress bar only updates after a chunk is finished.

    Returns:
    """
    if eval:
        download_eval_public(dataset_folder)
    download_audioset_data(
        dataset_folder,
        weak=weak,
        unlabel_in_domain=unlabel_in_domain,
        validation=validation,
        n_jobs=n_jobs,
        chunk_size=chunk_size,
    )
